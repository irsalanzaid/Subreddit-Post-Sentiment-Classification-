{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGnOppqJAo9A"
      },
      "source": [
        "# **Sentiment Analysis**\n",
        "\n",
        "##  Subreddit Post Sentiment Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw0iaZrAaE7o"
      },
      "source": [
        "#importing librarires\n",
        "import pandas as pd    #linear algebra\n",
        "import numpy as np     #data processing\n",
        "\n",
        "train_data = pd.read_json(\"https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_train.json\")\n",
        "\n",
        "validation_data = pd.read_json(\"https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_validation.json\")\n",
        "\n",
        "test_data = pd.read_json(\"https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_test.json\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "zUEElzPzBbIZ",
        "outputId": "9fbddaae-7ec7-434e-de1e-8e0b2904a1cd"
      },
      "source": [
        "train_data.head(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "      <th>majority_type</th>\n",
              "      <th>is_first_post</th>\n",
              "      <th>post_depth</th>\n",
              "      <th>in_reply_to</th>\n",
              "      <th>sentiment.polarity</th>\n",
              "      <th>sentiment.subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>relationships</td>\n",
              "      <td>My Friend/Crush [22/F] acting weird after I [2...</td>\n",
              "      <td>t1_cy7f317</td>\n",
              "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
              "      <td>Melodrama_</td>\n",
              "      <td>It's a sad realization, isn't it?</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>t1_cy7erc5</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>relationships</td>\n",
              "      <td>My Friend/Crush [22/F] acting weird after I [2...</td>\n",
              "      <td>t1_cy7hlyf</td>\n",
              "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
              "      <td>Melodrama_</td>\n",
              "      <td>I told her a couple of minutes ago that I didn...</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>t1_cy7erc5</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.483631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>relationships</td>\n",
              "      <td>My Friend/Crush [22/F] acting weird after I [2...</td>\n",
              "      <td>t1_cy7etrr</td>\n",
              "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
              "      <td>TreatYoSelves</td>\n",
              "      <td>Leeches don't make good friends.</td>\n",
              "      <td>answer</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>t3_3xshx9</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>relationships</td>\n",
              "      <td>My Friend/Crush [22/F] acting weird after I [2...</td>\n",
              "      <td>t1_cy7hhpq</td>\n",
              "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
              "      <td>Melodrama_</td>\n",
              "      <td>I just ended it. Apparently she wasn't a good ...</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>t1_cy7etrr</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>relationships</td>\n",
              "      <td>My Friend/Crush [22/F] acting weird after I [2...</td>\n",
              "      <td>t1_cy7q0qg</td>\n",
              "      <td>https://www.reddit.com/r/relationships/comment...</td>\n",
              "      <td>TreatYoSelves</td>\n",
              "      <td>Good for you!  Make sure you stick with it.</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>t1_cy7hhpq</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.744444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       subreddit  ... sentiment.subjectivity\n",
              "0  relationships  ...               1.000000\n",
              "1  relationships  ...               0.483631\n",
              "2  relationships  ...               0.600000\n",
              "3  relationships  ...               0.475000\n",
              "4  relationships  ...               0.744444\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7X6un9haG7c",
        "outputId": "4585b7ac-2c1e-4b3a-993c-c16e7fe5a109"
      },
      "source": [
        "#to extract information, pre-processing\n",
        "import spacy\n",
        "\n",
        "# Load the medium english model. \n",
        "# We will use this model to get embedding features for tokens later.\n",
        "#!python -m spacy download en_core_web_md\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "#importing nltk\n",
        "# Download a stopword list\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZHSfPh8aJ8W"
      },
      "source": [
        "#funtion to Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#funtion to Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    normalized = token.text.lower().strip()\n",
        "    if ((token.is_alpha or token.is_digit)):\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "  return normalized_tokens\n",
        "\n",
        "#Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcxzRpZXDaCw"
      },
      "source": [
        "#Q1\n",
        "Use the text from the reddit posts (Known as “body”) to train classification models using the Scikit Learn package.\n",
        "The labels to predict are the sentiment.polarity for each post. Conduct experiments using the following combinations\n",
        "of classifier models and feature representations:\n",
        "\n",
        "\n",
        "(a) Dummy Classifier with strategy=\"most_frequent\"\n",
        "\n",
        "(b) Dummy Classifier with strategy=\"stratified\"\n",
        "\n",
        "(c) LogisticRegression with One-hot vectorization\n",
        "\n",
        "(d) LogisticRegression with TF-IDF vectorization (default settings)\n",
        "\n",
        "(e) SVC Classifier with One-hot vectorization (SVM with RBF kernel, default settings))\n",
        "\n",
        "\n",
        "(f) An ‘interesting’ classifier model and vectorization of your choice with appropriate pre-processing New Section\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCmsvWBfaNCm"
      },
      "source": [
        "#Converting into Bag of Words(vectorization)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Pass in the tokenizer as the tokenizer to the vectorizer.\n",
        "# Create a one-hot encoding vectorizer.\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize_normalize, binary=True)\n",
        "train_features = vectorizer.fit_transform(train_data['body'])\n",
        "\n",
        "# This creates input features for our classification on all subsets of our collection.\n",
        "validation_features = vectorizer.transform(validation_data['body'])\n",
        "test_features = vectorizer.transform(test_data['body'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va4LE3Y4aO8r"
      },
      "source": [
        "\n",
        "train_labels = train_data['sentiment.polarity']\n",
        "validation_labels = validation_data['sentiment.polarity']\n",
        "test_labels = test_data['sentiment.polarity']\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJzdU61KaR9j"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels,average=None)\n",
        "  recall = recall_score(predictions, true_labels,average=None)\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1,average=None) #1 means f_1 measure\n",
        "  print(\"Classifier has Acc=\", (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3, zero_division = 0))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions)) # Note the order here is true, predicted\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFdoy3uQaUDw",
        "outputId": "f67817c5-f443-4e42-ad5d-6e9b98ed8482"
      },
      "source": [
        "##Solution (a)\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "\n",
        "dummy_mf = DummyClassifier(strategy='most_frequent')\n",
        "dummy_mf.fit(train_features, train_labels)\n",
        "print(dummy_mf.score(validation_features, validation_labels))\n",
        "evaluation_summary(\"Dummy Majority\", dummy_mf.predict(validation_features), validation_labels)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6307494371180444\n",
            "Evaluation for: Dummy Majority\n",
            "Classifier has Acc= ('Dummy Majority', 0.6307494371180444, array([0., 1., 0., 0., 0.]), array([0.        , 0.63074944, 0.        , 0.        , 0.        ]), array([0.        , 0.77357002, 0.        , 0.        , 0.        ]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.000     0.000     0.000         0\n",
            "      neutral      1.000     0.631     0.774      3109\n",
            "     positive      0.000     0.000     0.000         0\n",
            "very negative      0.000     0.000     0.000         0\n",
            "very positive      0.000     0.000     0.000         0\n",
            "\n",
            "     accuracy                          0.631      3109\n",
            "    macro avg      0.200     0.126     0.155      3109\n",
            " weighted avg      1.000     0.631     0.774      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   0  215    0    0    0]\n",
            " [   0 1961    0    0    0]\n",
            " [   0  845    0    0    0]\n",
            " [   0   15    0    0    0]\n",
            " [   0   73    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TIhNHTzaW4J",
        "outputId": "45ca7a7b-5c58-4065-b9e7-b3ad635c3f52"
      },
      "source": [
        "#solution (b)\n",
        "dummy_prior = DummyClassifier(strategy='stratified')\n",
        "dummy_prior.fit(train_features, train_labels)\n",
        "print(dummy_prior.score(validation_features, validation_labels))\n",
        "evaluation_summary(\"Dummy Prior\", dummy_prior.predict(validation_features), validation_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4509488581537472\n",
            "Evaluation for: Dummy Prior\n",
            "Classifier has Acc= ('Dummy Prior', 0.473142489546478, array([0.06511628, 0.63691994, 0.24497041, 0.        , 0.01369863]), array([0.06034483, 0.61954365, 0.26744186, 0.        , 0.01785714]), array([0.06263982, 0.62811164, 0.2557134 , 0.        , 0.01550388]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.065     0.060     0.063       232\n",
            "      neutral      0.637     0.620     0.628      2016\n",
            "     positive      0.245     0.267     0.256       774\n",
            "very negative      0.000     0.000     0.000        31\n",
            "very positive      0.014     0.018     0.016        56\n",
            "\n",
            "     accuracy                          0.473      3109\n",
            "    macro avg      0.192     0.193     0.192      3109\n",
            " weighted avg      0.479     0.473     0.476      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[  14  139   58    3    1]\n",
            " [ 154 1249  492   24   42]\n",
            " [  59  564  207    3   12]\n",
            " [   0   12    3    0    0]\n",
            " [   5   52   14    1    1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIIDLbteae2C",
        "outputId": "175b76b6-90fb-4269-a640-2f6245afea5c"
      },
      "source": [
        "##Solution (c)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver='saga', max_iter = 1000)\n",
        "lr_model = lr.fit(train_features, train_labels)\n",
        "evaluation_summary(\"LR onehot\", lr_model.predict(validation_features), validation_labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: LR onehot\n",
            "Classifier has Acc= ('LR onehot', 0.752010292698617, array([0.26046512, 0.8827129 , 0.61893491, 0.2       , 0.34246575]), array([0.51851852, 0.78148984, 0.70201342, 0.42857143, 0.73529412]), array([0.34674923, 0.82902299, 0.65786164, 0.27272727, 0.46728972]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.260     0.519     0.347       108\n",
            "      neutral      0.883     0.781     0.829      2215\n",
            "     positive      0.619     0.702     0.658       745\n",
            "very negative      0.200     0.429     0.273         7\n",
            "very positive      0.342     0.735     0.467        34\n",
            "\n",
            "     accuracy                          0.752      3109\n",
            "    macro avg      0.461     0.633     0.515      3109\n",
            " weighted avg      0.790     0.752     0.766      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[  56  151    7    1    0]\n",
            " [  46 1731  176    3    5]\n",
            " [   5  313  523    0    4]\n",
            " [   1   11    0    3    0]\n",
            " [   0    9   39    0   25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNrX_pHlahQx"
      },
      "source": [
        "#solution (d)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "TFIDF_Vectoriser=TfidfVectorizer( binary=True, max_features = 5000 ) "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1sQ6AOValYq"
      },
      "source": [
        "train_features_TFIDF = TFIDF_Vectoriser.fit_transform(train_data['body'])\n",
        "\n",
        "# This creates input features for our classification on all subsets of our collection.\n",
        "validation_features_TFIDF = TFIDF_Vectoriser.transform(validation_data['body'])\n",
        "test_features_TFIDF = TFIDF_Vectoriser.transform(test_data['body'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3RT9C7Oam11",
        "outputId": "6d71274e-a04f-45c5-a90c-247f52648509"
      },
      "source": [
        "lr = LogisticRegression(solver='saga')\n",
        "combined_model = lr.fit(train_features_TFIDF,train_labels)\n",
        "evaluation_summary(\"LR TFIDF\", lr.predict(validation_features_TFIDF), validation_labels) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: LR TFIDF\n",
            "Classifier has Acc= ('LR TFIDF', 0.7378578321003538, array([0.11162791, 0.93013768, 0.51597633, 0.        , 0.1369863 ]), array([0.68571429, 0.74297352, 0.71947195, 0.        , 0.76923077]), array([0.192     , 0.82608696, 0.60096485, 0.        , 0.23255814]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.112     0.686     0.192        35\n",
            "      neutral      0.930     0.743     0.826      2455\n",
            "     positive      0.516     0.719     0.601       606\n",
            "very negative      0.000     0.000     0.000         0\n",
            "very positive      0.137     0.769     0.233        13\n",
            "\n",
            "     accuracy                          0.738      3109\n",
            "    macro avg      0.339     0.583     0.370      3109\n",
            " weighted avg      0.837     0.738     0.773      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[  24  185    6    0    0]\n",
            " [   8 1824  127    0    2]\n",
            " [   1  407  436    0    1]\n",
            " [   2   13    0    0    0]\n",
            " [   0   26   37    0   10]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NwHFEJJaozy",
        "outputId": "1392727b-8d9e-4a6e-a581-caa12a3bd464"
      },
      "source": [
        "#solution (e)\n",
        "from sklearn.svm import SVC\n",
        "sv= SVC()\n",
        "svc_model = sv.fit(train_features, train_labels)\n",
        "evaluation_summary(\"LR onehot\", svc_model.predict(validation_features), validation_labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: LR onehot\n",
            "Classifier has Acc= ('LR onehot', 0.7224187841749758, array([0.01860465, 0.94390617, 0.46272189, 0.        , 0.        ]), array([0.66666667, 0.72107519, 0.72947761, 0.        , 0.        ]), array([0.0361991 , 0.81757951, 0.56625634, 0.        , 0.        ]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.019     0.667     0.036         6\n",
            "      neutral      0.944     0.721     0.818      2567\n",
            "     positive      0.463     0.729     0.566       536\n",
            "very negative      0.000     0.000     0.000         0\n",
            "very positive      0.000     0.000     0.000         0\n",
            "\n",
            "     accuracy                          0.722      3109\n",
            "    macro avg      0.285     0.423     0.284      3109\n",
            " weighted avg      0.859     0.722     0.773      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   4  208    3    0    0]\n",
            " [   1 1851  109    0    0]\n",
            " [   1  453  391    0    0]\n",
            " [   0   15    0    0    0]\n",
            " [   0   40   33    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTryogKNarCq",
        "outputId": "18bfea72-492f-4d2c-8a06-75e27ccc532f"
      },
      "source": [
        "#Solution (f)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "nb_model = classifier.fit(train_features, train_labels)\n",
        "\n",
        "validation_predicted_labels = nb_model.predict(validation_features)\n",
        "evaluation_summary(\"One-hot NB\",  validation_predicted_labels, validation_labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: One-hot NB\n",
            "Classifier has Acc= ('One-hot NB', 0.6773882277259569, array([0.00930233, 0.92197858, 0.35029586, 0.        , 0.        ]), array([0.16666667, 0.6980695 , 0.58382643, 0.        , 0.        ]), array([0.01762115, 0.79455065, 0.43786982, 0.        , 0.        ]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.009     0.167     0.018        12\n",
            "      neutral      0.922     0.698     0.795      2590\n",
            "     positive      0.350     0.584     0.438       507\n",
            "very negative      0.000     0.000     0.000         0\n",
            "very positive      0.000     0.000     0.000         0\n",
            "\n",
            "     accuracy                          0.677      3109\n",
            "    macro avg      0.256     0.290     0.250      3109\n",
            " weighted avg      0.825     0.677     0.733      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   2  202   11    0    0]\n",
            " [   9 1808  144    0    0]\n",
            " [   0  549  296    0    0]\n",
            " [   1   12    2    0    0]\n",
            " [   0   19   54    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JpaxGQcGtlz"
      },
      "source": [
        "# Q2\n",
        "In this task you will improve the effectiveness of the LogisticRegression with TF-IDF vectorization from Q1.\n",
        "\n",
        "i) **Parameter tuning** - Tune the parameters for both the vectorizer and classifier on the validation set (or using\n",
        "CV-fold validation on the train).\n",
        "\n",
        "● Classifier - Regularization C value (typical values might be powers of 10 (from 10^-3 to 10^5)\n",
        "\n",
        "● Vectorizer - Parameters: sublinear_tf and max_features (vocabulary size) (in a range None to 50k)\n",
        "\n",
        "● Select another parameter of your choice from the classifier or vectorizer\n",
        "\n",
        "\n",
        "ii) **Error analysis** - Manually examine the predictions of your optimised classifier on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I02ZLJkas5N"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def evaluation_summary1(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels,average='micro')\n",
        "  recall = recall_score(predictions, true_labels,average=None)\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1,average=None) #1 means f_1 measure\n",
        "  print(\"Classifier has Acc=\", (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3, zero_division = 0))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions)) # Note the order here is true, predicted\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r-J0wV_ax_E"
      },
      "source": [
        "#solution (i)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "TFIDF_Vectoriser=TfidfVectorizer( binary=True,sublinear_tf = False, max_features = 1212 ) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idgWZi2-az-Q"
      },
      "source": [
        "train_features_TFIDF = TFIDF_Vectoriser.fit_transform(train_data['body'])\n",
        "\n",
        "# This creates input features for our classification on all subsets of our collection.\n",
        "validation_features_TFIDF = TFIDF_Vectoriser.transform(validation_data['body'])\n",
        "test_features_TFIDF = TFIDF_Vectoriser.transform(test_data['body'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SepBnRrMa1rs",
        "outputId": "b53815f6-01c1-41c3-a72f-25c4d638e766"
      },
      "source": [
        "lr = LogisticRegression(solver='saga',C=10,penalty='l2')\n",
        "combined_model = lr.fit(train_features_TFIDF,train_labels)\n",
        "evaluation_summary1(\"LR TFIDF\", lr.predict(validation_features_TFIDF), validation_labels) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: LR TFIDF\n",
            "Classifier has Acc= ('LR TFIDF', 0.7671276937922161, 0.7671276937922161, array([0.54330709, 0.78843568, 0.74717514, 0.5       , 0.67567568]), array([0.40350877, 0.83921756, 0.68126207, 0.28571429, 0.45454545]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.321     0.543     0.404       127\n",
            "      neutral      0.897     0.788     0.839      2231\n",
            "     positive      0.626     0.747     0.681       708\n",
            "very negative      0.200     0.500     0.286         6\n",
            "very positive      0.342     0.676     0.455        37\n",
            "\n",
            "     accuracy                          0.767      3109\n",
            "    macro avg      0.477     0.651     0.533      3109\n",
            " weighted avg      0.804     0.767     0.780      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[  69  138    7    1    0]\n",
            " [  50 1759  144    2    6]\n",
            " [   5  305  529    0    6]\n",
            " [   3    9    0    3    0]\n",
            " [   0   20   28    0   25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nVykCzIa3b2",
        "outputId": "991eccd5-3bc0-45eb-8fcc-46270b9e0516"
      },
      "source": [
        "#solution (ii)\n",
        "lr = LogisticRegression(solver='saga',C=10,penalty='l1')\n",
        "combined_model = lr.fit(train_features_TFIDF,train_labels)\n",
        "evaluation_summary1(\"LR TFIDF\", lr.predict(test_features_TFIDF), test_labels) "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: LR TFIDF\n",
            "Classifier has Acc= ('LR TFIDF', 0.7617031872509961, 0.7617031872509961, array([0.50438596, 0.80612626, 0.74025974, 0.33333333, 0.4375    ]), array([0.45098039, 0.83143903, 0.70470756, 0.32258065, 0.42168675]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.408     0.504     0.451       228\n",
            "      neutral      0.858     0.806     0.831      2677\n",
            "     positive      0.672     0.740     0.705      1001\n",
            "very negative      0.312     0.333     0.323        30\n",
            "very positive      0.407     0.438     0.422        80\n",
            "\n",
            "     accuracy                          0.762      4016\n",
            "    macro avg      0.532     0.564     0.546      4016\n",
            " weighted avg      0.773     0.762     0.766      4016\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 115  155    6    5    1]\n",
            " [ 103 2158  224   13   16]\n",
            " [   4  327  741    2   28]\n",
            " [   6   16    0   10    0]\n",
            " [   0   21   30    0   35]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GalIculrHosi"
      },
      "source": [
        "# Q3\n",
        "In this task your goal is to add two features to (try to) improve sentiment polarity classification performance obtained\n",
        "in Q2.3\n",
        "You must implement and describe two new classifier features and add them to the tuned model from Q2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbP7cHmea9fD"
      },
      "source": [
        "#solution (3)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def evaluation_summary1(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels,average=None)\n",
        "  recall = recall_score(predictions, true_labels,average=None)\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1,average=None) #1 means f_1 measure\n",
        "  print(\"Classifier has Acc=\", (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3, zero_division = 0))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions)) # Note the order here is true, predicted\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP5p33EKa_HL"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "TFIDF_Vectoriser=TfidfVectorizer( binary=True,sublinear_tf = False , max_features = 1000 ) "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5OZ2yqnbA0m"
      },
      "source": [
        "train_features_TFIDF = TFIDF_Vectoriser.fit_transform(train_data['in_reply_to'])\n",
        "\n",
        "# This creates input features for our classification on all subsets of our collection.\n",
        "validation_features_TFIDF = TFIDF_Vectoriser.transform(validation_data['in_reply_to'])\n",
        "test_features_TFIDF = TFIDF_Vectoriser.transform(test_data['in_reply_to'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AxkH_DFbCTV",
        "outputId": "a10e6455-30aa-41a3-a490-2d71bd0c44b0"
      },
      "source": [
        "lr = LogisticRegression(solver='saga',C=10,penalty='l2')\n",
        "combined_model = lr.fit(train_features_TFIDF,train_labels)\n",
        "evaluation_summary1(\"LR TFIDF\", lr.predict(validation_features_TFIDF), validation_labels) "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: LR TFIDF\n",
            "Classifier has Acc= ('LR TFIDF', 0.6307494371180444, array([0., 1., 0., 0., 0.]), array([0.        , 0.63074944, 0.        , 0.        , 0.        ]), array([0.        , 0.77357002, 0.        , 0.        , 0.        ]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.000     0.000     0.000         0\n",
            "      neutral      1.000     0.631     0.774      3109\n",
            "     positive      0.000     0.000     0.000         0\n",
            "very negative      0.000     0.000     0.000         0\n",
            "very positive      0.000     0.000     0.000         0\n",
            "\n",
            "     accuracy                          0.631      3109\n",
            "    macro avg      0.200     0.126     0.155      3109\n",
            " weighted avg      1.000     0.631     0.774      3109\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   0  215    0    0    0]\n",
            " [   0 1961    0    0    0]\n",
            " [   0  845    0    0    0]\n",
            " [   0   15    0    0    0]\n",
            " [   0   73    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgn1NAQ5bEI5"
      },
      "source": [
        "train_features_TFIDF = TFIDF_Vectoriser.fit_transform(train_data['in_reply_to'])\n",
        "\n",
        "# This creates input features for our classification on all subsets of our collection.\n",
        "validation_features_TFIDF = TFIDF_Vectoriser.transform(validation_data['in_reply_to'])\n",
        "test_features_TFIDF = TFIDF_Vectoriser.transform(test_data['in_reply_to'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHKQYqLubIK9",
        "outputId": "acb54525-b631-4009-8ede-bb1a76b73f54"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "RFC = RandomForestClassifier(n_estimators=100,min_samples_leaf=2)\n",
        "combined_model = RFC.fit(train_features_TFIDF,train_labels)\n",
        "evaluation_summary1(\"LR TFIDF\", RFC.predict(test_features_TFIDF), test_labels) "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for: LR TFIDF\n",
            "Classifier has Acc= ('LR TFIDF', 0.625996015936255, array([0., 1., 0., 0., 0.]), array([0.        , 0.62599602, 0.        , 0.        , 0.        ]), array([0.        , 0.76998469, 0.        , 0.        , 0.        ]))\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negative      0.000     0.000     0.000         0\n",
            "      neutral      1.000     0.626     0.770      4016\n",
            "     positive      0.000     0.000     0.000         0\n",
            "very negative      0.000     0.000     0.000         0\n",
            "very positive      0.000     0.000     0.000         0\n",
            "\n",
            "     accuracy                          0.626      4016\n",
            "    macro avg      0.200     0.125     0.154      4016\n",
            " weighted avg      1.000     0.626     0.770      4016\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   0  282    0    0    0]\n",
            " [   0 2514    0    0    0]\n",
            " [   0 1102    0    0    0]\n",
            " [   0   32    0    0    0]\n",
            " [   0   86    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMS47L6GIArx"
      },
      "source": [
        "# Observations\n",
        "\n",
        "The data has a neutral overall tone. Around 3000 posts are filled with good feelings. There are less than 1000 posts with \"very positive\" sentiments. The most recent post has received \"very negative\" feedback.\n",
        "\n",
        "\n",
        "The data is split into three parts. Training, testing and validating. The model is based on the training data and the performance of the data is evaluated by predicting the test data. The different technology used is the virtual classifier, and logistic regression has the highest precision compared to other technologies. Use hot vector logistic regression, use TF IDF vector logistic regression, svc classifier and naive Bayes. The highest precision of the model classifier is found in logistic regression. One of the heat vectors has a precision of 75%, that is, the total percentage of the total correlation is 46%, and the total percentage after the classifier correctly obtains the total correlation is 63%. TF IDF has an overall accuracy of 73% and an accuracy of 33, and the F1 recovery rates are 58% and 37%. The virtual and out-of-tune classifiers and the most common strategies have an overall accuracy of about 63%. The hierarchical strategy virtual classifier has the lowest accuracy rate of 48%, and the accuracy rate, retrieval rate, and F1 rate are 19%, 19% and 19%.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The classifier of choice for the model and vectorization is Naive Bayes, because this algorithm is the best technique to use in classifying tests with multiple classes of variables. Compared to the performance of the different previous models, NB I will go to them. Its performance is better than the virtual classifier, but the precision is lower than the logic model and the CVC classifier, because the overall precision of the model is 67.7%, but the precision and recovery are better than the model with reduced precision and recovery by 25% and 29%, respectively, with an F1 score of 25%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0RXebF_I3Ol"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}